<!DOCTYPE html>
<html>
  <head>
    <title>Clustering and Mixture Models</title>
    <meta charset="utf-8">
        <style>
       @import url(https://fonts.googleapis.com/css?family=Garamond);
       @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
       @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
     </style>
     <link rel="stylesheet" href="../style.css">
  </head>
  <body>
    <textarea id="source">

class: center, middle

### W4995 Applied Machine Learning

# Clustering

03/27/19

Andreas C. Müller

(Adapted and modified for CC 6021236 @ PCC/Ciencias/UCV by 

Eugenio Scalise, October 2019)


???
Today we're gonna talk about clustering and mixture models,
mostly clustering algorithms. Next time we'll talk about
evaluation of clustering.

FIXME: illustration of Kmeans++
FIXME: linkage explanation diagram
FIXME: cluster sized in particularly good for dbscan
debugging
---

class: center, middle

# Clustering

???



---
.center[
![:scale 70%](images/cluster_intro_1.png)
]

???
The ideas is that you start out with a bunch of data points,
and the assumption is that they fall into groups or
clusters, and the goal is to discover these underlying
groups.


---
.center[
![:scale 70%](images/cluster_intro_2.png)
]

???
For example you want to find that these points belong to the
same groups, and these, and these.
---
.center[
![:scale 70%](images/cluster_intro_3.png)
]
???
Usually the clusters are identified by a cluster label or cluster indicator,
though these numbers are arbitrary. What you care about is which points belong to the same cluster.
So I could relabel the clusters and this would still be considered the same clustering.


What we're really interested in is the partitioning of the
data, though the way it's implemented in the scikit-learn
API is that you predict an integer for each data point, so
the clusters are numbered 0.. n_clusters -1. So we'd label
these points with 0, these with one, and so on. But you
should keep in mind that the number themselves have no
meaning, renumbering the clusters corresponds to the same
clustering.  We're really only interested in which points
have the same labels and which points have different labels.
---

class: spacious
# Clustering

- Partition data into groups (clusters)
- Points within a cluster should be “similar”.
- Points in different cluster should be “different”.

???


So you want to partition the data, so that each data point
belongs to exactly one of the groups. And you want to define
groups such that points in the same cluster are similar, and
points in different clusters are different. And the
different algorithms define how they measure similarity in
different ways.


- Number of clusters might be pre-specified. - Notations of
“similar” and “different” depend on algorithm or user specified metrics.
- Identify groups by assigning “cluster labels” to each data point. FIXME!
---
.center[
![:scale 70%](images/cluster_intro_3.png)
]

???
The cluster algorithms have many parameters, and one of the
most common ones is the number of clusters. In this dataset
I drew here, it's quite obvious that there are three groups,
and if it's this obvious there is usually a way to figure
out how many groups there are. But for many algorithms, you
need to specify the number of groups in advance.

---

.center[
![:scale 70%](images/cluster_intro_4.png)
]

???
Group into 4 clusters


If you misspecify the number of groups you'd get something
like this. To pick the number of clusters, in general you either need to have
some prior knowledge about the data, or you need to inspect
the results manually to see if the parameters are good.

Often, you don't know the number of clusters, or if there
are even clusters in the data. Very rarely is the case as
clear as in this example.  Still there might be a reasonable
grouping, so there is no "right" amount of clusters, but you
might still get something out of the clustering.
---

# Goals of Clustering

- Data Exploration
  - Are there coherent groups ?
  - How many groups are there ?

--

- Data Partitioning
  - Divide data by group before further processing

--

- Unsupervised feature extraction
  - Derive features from clusters or cluster distances

???
So why would we want to use a clustering algorithm? The
main usage that I've come across is exporation of the data
and understanding the structure of the data. Are there
similar groups within the data? What are these? How many are
there?  You can also use clustering to summarize or compress
the data by representing each group by a single prototype,
say the center. You can also use clustering if you want to
divide your data, for example if you want to know different
parts of the dataset behave fundamentally differently, so
you could for example cluster a dataset, and then apply,
say, a linear model, to each cluster. That would create a
more powerfull model than just learning a linear model on
the whole dataset.


Another use is unsupervised feature extraction. You can also
use clustering to derive a different representation of the
data.

An often cited application of clustering is customer
segmentation, though I'd say data exploration or feature
extraction are more common.
---

class: center, middle

# Clustering Algorithms

???
So now let's start discussing some of the standard
algorithms.
---

class: center, middle

# K-Means

???
One of the most basic and most well-known ones is kmeans.
Here k stands for the number of clusters you're trying to
find.

---
# Objective function for K-Means
<br \>
<br \>
`$$\Large \min_{\mathbf{c}_j \in \mathbf{R}^p, j=1,..,k} \sum_i ||\mathbf{x}_i - \mathbf{c}_{x_i}||^2 $$`
$\mathbf{c}_{x_i}$ is the cluster center $c_j$ closest to $x_i$

- This is an NP hard problem, so we can't really hope to solve it exactly (k-means algorithm provide an aproximation).
???
- Finds a local minimum of minimizing squared distances (exact solution is
NP hard):
- New data points can be assigned cluster membership based on existing clusters.

Here is what this algorithm optimizes, the
objective function.

It's the sum of the distances of the cluster centers to all
the points in the clusters.

So the clusters centers are c_j and we want to find cluster
centers such that the sum of the distance of each point to
its closest cluster center is minimized.

This is an NP hard problem, so we can't really hope to solve
it exactly, but we can run the k-means algorithm that we'll discussed,
and it will provide us with some local minimum of this objective.


---
# K-Means algorithm
.wide-left-column[
.center[
![:scale 100%](images/kmeans.png)
]
]

.narrow-right-column[
.smaller[
- Pick number of clusters k.
- Pick k random points as
“cluster center”
- While cluster centers change:
  1. Assign each data point to it’s closest cluster center
  2. Recompute cluster centers as the mean of the assigned points.
]
]

???
The algorithm proceeds like this: you pick the number of
clusters k, then you randomly pick k data points from the
data, and declare those as cluster centers. Then, you label
each data point according to which cluster center it is
closest to. Finally, you recompute the cluster centers as
the means of the clusters, and you iterate. The algorithm
always converges, which is when the assignment of the points
to the clusters doesn't change any more. Often you might
want to not wait until full convergence, but just stop if
the cluster centers don't move much between iterations.

So you can see the green cluster center moving towards the
red here until it has captured all the points in this group.
Questions? Who has seen this before?

There's a couple of nice things about this algorithm;

It's very easy to write down and understand. Each cluster is
represented by a center, which makes things simple, and you
can reassign any new point you observe to a cluster by just
computing the distances to all the cluster centers.

So in this case, if we build a model on some data that we
collected, we can easily apply this clustering to new data
points. That's not the case for all clustering algorithms.
---


# K-Means API


.narrow-right-column[
.center[
![:scale 100%](images/kmeans_api.png)
]
]

.wide-left-column[
.smaller[
```python
X, y = make_blobs(centers=4, random_state=1)

km = KMeans(n_clusters=5, random_state=0)
km.fit(X)
print(km.cluster_centers_.shape)
print(km.labels_.shape)
```
```
(5, 2)
(100,)
```
```python
# predict is the same as labels_ on training data
# but can be applied to new data
print(km.predict(X).shape)
```
```
(100,)
```
```python
plt.scatter(X[:, 0], X[:, 1], c=km.labels_)
```

]
]

???
In scikit-learn, I’ve made some random data points. I
instantiated a KMeans object, passing the number of clusters
I want and for reproducibility I set the random state to 0,
I implemented fit, and then I inspected some of attributes.
So the shape of cluster centers is 5 and 2. Meaning there’s
5 cluster centers in 2-dimensions. The labels are the
cluster indices assigned to the data. So there's 100 data
points and each of them is assigned an integer from zero to
four.

KMeans also has a predict method. The predict method
computes the cluster labels according to this clustering. So
this is a nice property of KMeans that you can assign
cluster labels to new data points, via this predict methods.

If you call predict method on the training dataset, you get
the same thing back that is in labels, which is just a
training data labels. But you can also pass in any new data
that you observe, and it will assign one of the clusters
based on which cluster centers are closest.

Q: Why is the centers in two dimensions?

Because the input spaces is in two dimensions in this toy
dataset.

Cluster centers live in the same space as the data.

---

# Restriction of Cluster Shapes
.left-column[
![:scale 85%](images/cluster_shapes_1.png)
]
.right-column[
![:scale 100%](images/cluster_shapes_2.png)
]
.reset-column[
Clusters are Voronoi-diagrams of centers (convex sets)
]

???
I want to talk about some of the properties and limitations
of KMeans.

So the cluster shapes, I restricted them in KMeans because
they're Voronoi-diagram of the centers.

Voronoi-diagrams basically just mean these cells… while the
boundary between two neighboring clusters is just the center
of the two centers. So given these centers completely define
a clustering and so you compute this distance boundaries.
The whole clustering is specified just by the cluster
centers, but that really restricts the shape. So all of
these cells will be convex sets, so you can't have like a
banana shape or something like that. That's one of the
limitations.

- Clusters are always convex in space


---

# Limitations of K-Means

.center[
![:scale 60%](images/kmeans_limitations_1.png)
]

- Cluster boundaries equidistant to centers

???
You can also not really model the size of centers very well.
So here, I have three clusters and this is the result of
KMeans on this dataset that I created from three Gaussians.
So the Gaussians I used to create this data set was one big
blob in the center and one smaller ones in bottom left
corner and top right corner.

But KMeans wasn't really able to pick up on these three
clusters because the boundary between the clusters is the
middle between the cluster centers.


---

# Limitations of K-Means

.center[
![:scale 60%](images/kmeans_limitations_2.png)
]

 - Can’t model covariances well

???
It's also not able to capture covariance. So here, for
example, there's three groups in this dataset, and KMeans
doesn't really pick up on these shapes.


---
# Limitations of K-Means

.center[
![:scale 60%](images/kmeans_limitations_3.png)
]

- Only simple cluster shapes

???
Since you have only simple cluster shapes, so if I use this
two cluster dataset, this will be what KMeans gives me
because basically it cannot do anything else but straight
lines for two clusters.

Even though it has these restrictions it’s still a very
commonly used algorithm. And so we'll talk a little bit more
about some of the other properties of this algorithm.

---

class: center, middle

# Agglomerative Clustering

???
The next algorithm I want to talk about is similarly
classical, it’s called agglomerative clustering.
---

# Agglomerative Clustering

- Start with all points in their own cluster.
- Greedily merge the two most similar clusters.

.center[
![:scale 80%](images/agglomerative_clustering.png)
]

???
In agglomerative clustering, the idea is that you start out
with a bunch of points again. You assign each point its own
cluster label, and then you greedily merge clusters that are
similar. So here, each of the points is its own cluster,
then, in step one, I'm merge the two closest points. Step
two, I merged the next closest one, and so on. I can keep
merging and merging until one big group is left. Here, I
merged until there's three clusters left.



- Creates a hierarchical clustering from with cluster
sizes from n_samples to single cluster.

---
class: spacious

# Dendograms

.left-column[
![:scale 100%](images/dendograms_1.png)
]

.left-column[
![:scale 80%](images/dendograms_2.png)
]

???
Dendogram is another visualization of agglomerative
clustering. Here, given this procedure, you can visualize
this as a tree. It is also called as hierarchical
clustering, because it gives you a whole hierarchy of
different clusters. On the lowest level of the hierarchy,
each data point is its own cluster center.

So here, I have 12 data points so I would have 12 clusters.
And if I merged everything, I have one cluster. But in
between, there's this whole hierarchy of different numbers
of clusters. Here, you can see the one and four was merged
first, then six and nine was merged, then two and eight was
merged, then zero and eleven, then five and zero and eleven
and so on.

The length of these bars are basically the distance between
the clusters that got merged. So you can see the distance
between zero and eleven, and zero, eleven, and five are very
small. So that basically means that all three of them have
almost the same distance. Whereas here, you can see one and
four are much closer together than one and four and three
and two.

By looking at this dendogram, it’s very helpful for picking
the number of clusters. So if I look at this, I would
probably say, three is the right number of clusters.

So for agglomerative clustering, you can either pick the
number of clusters or you can basically pick a threshold on
the merging criteria. So basically, if the clusters are too
far away, you stop merging. But you need to specify one of
these two things, either number of clusters, or when should
you stop merging. But for both, looking at Dendogram is
quite useful.

Agglomerative clustering is in scikit learn while Dendogram
are not available. But Dendogram can be computed in scipy
(code available in the GitHub repository)

Dendograms are mostly helpful if you don't have many data
points. If you have millions of data points, then this tree
will not be very easy to look at.

This is a bottom-up approach where you start merging things
from the bottom. You can also start by splitting the data or
compute in a top-down manner as well. Some of these are
equivalent and it depends on how you do similarity. Some of
the bottom up and top down approach lead to the same result.
And it depends mostly on how you're define the similarities.


Done with scipy!
FIXME dendrogram code!
---
# Linkage Criteria

.center[
![:scale 100%](images/merging_criteria.png)
]

.narrow-left-column[
.smaller[
Cluster sizes:
```
single :   [96  1  1  1  1]
average :  [82  9  7  1  1]
complete : [50 24 14 11  1]
ward :     [31 30 20 10  9]
```
]]

.wide-right-column[
.smallest[
- Single Linkage
  - Smallest minimum distance
- Average Linkage
  - Smallest average distance between all pairs in the clusters
- Complete Linkage
  - Smallest maximum distance
- Ward (default in sklearn)
  - Smallest increase in within-cluster variance
  - Leads to more equally sized clusters.

]]
???
Here's an example on some synthetic 2D data set. So here, I
run three different clustering algorithms. You can see that
here in average to data point being their own cluster where
as in ward the cluster size are much more similar.

In particular, average, complete and single linkage tend to
cut off single points that are very far away. So if one
point is far right from everything, every linkage criteria
except ward will get their own cluster.

There's four commonly used similarities. Complete linkage
looks at the smallest maximum distance. So if you look
between two clusters, you look at the maximum distance
between points in these two clusters.

Average linkage looks at the average distance of points
between clusters.

Single linkage looks at the minimum distance between
clusters.

Ward looks at smallest increase within cluster variance. You
merge the clusters and you look how much did the variance
increase.

Single linkage is the same as computing a minimum spanning
tree, and then cutting the longest edge. So that's actually
equivalent to top down approach.

Ward is the default in scikit-learn. It’s kind of nice
because it makes relatively equally sized clusters.


single linkage = cutting longest edge in minimum spanning tree.
---
class: center, middle

# DBSCAN 
(density- based spatial clustering of applications with noise)

???
The last clustering algorithm I want to talk about is
DBSCAN. This was developed in the 2000s. It's little bit
more complicated, but it's actually quite useful in
practice.
---

# Algorithm
.left-column[
![:scale 100%](images/DBSCAN-Illustration.svg)
]
.right-column[
- eps: neighborhood radius
- min_samples: 4
<br /><br />

- A: Core
- B, C: not core
- N: noise

<br />
Core: sample that has more than (min_sample - 1) other samples in its epsilon
neighborhood.

]
???
It's also an iterative algorithm.
This algorithm has two parameters, one is the neighborhood
radius called epsilon and the other is the minimum number of
samples in a cluster.

So here in this example, the colored points are data
points, and we've drawn the radius epsilon around it. That
said the minimum number of samples is four. So this means
each cluster is restricted to have a minimum of four samples
to be a cluster.

So given these parameters, we can define the notion of a
core sample. Core sample in DBSCAN is a sample that has more
than min_sample minus one other samples in its epsilon
neighborhood.

The way this algorithm works is, it picks a point at random.
Let's say, we pick point A at random, then it looks whether
this is a core point or not, by looking at the number of
neighbors with an epsilon, you see three neighbors here with
an epsilon and hence it's a core point and so that means I
give it a new cluster label, in this case, it’s red. Then I
add all of these neighbors to a queue and then I process
them all in turn. So I go to this red point, check is it a
core sample and not a core sample and again, it has four
neighbors, so it's a core point. And I added all the
neighbors to the queue. And so you keep doing that.

Here, once you reach point B, every point that you're going
to reach within an epsilon neighborhood of the cluster will
get the same cluster label as A. So if you reach this point,
you will see the epsilon neighborhood, this point B wasn't
visited yet and you will assign that the same cluster label.
But B itself is not a core point and so you stop iterating
there. Same goes for C.

So once your queue is empty, meaning you went through all of
these points, all the core points that you could reach, then
you pick the next point. If you pick a point like this N,
this is not a core point. And so if you visit it, it has no
enough samples in the neighborhood, you label it as noise.
One kind of interesting thing about this, we label it as
noise for now, basically, if at the very first point I pick
B, I would have labeled B as noise in the beginning, since B
is not a core point so it will not get its own cluster. So I
would label B as noise. But then later on, when I label the
red cluster, I see I can reach B from the red cluster, I’ll
give it that cluster label.

If there was an unexplored neighbor here, we would not visit
it because only core points basically add points to the
queue. So if there was two unexplored neighbors, it would be
a core point, if there's only one, it's not a core point
because it doesn't have enough neighbors. So basically, the
cluster doesn't promote in that direction.

One thing that’s nice about this is you identify noise
points. The other clustering algorithm didn’t have a concept
of noise points. In DBSCAN, all points are assigned to
cluster or they’re labeled noise, depending on what you want
to have on your clustering algorithm that might be good or
bad.



- Clusters are formed by “core samples”
- Sample is “core sample” if more than min_samples is
within epsilon - “dense region”
- Start with a core sample
- Recursively walk neighbors that are core-samples and
add to cluster.
- Also add samples within epsilon that are not core
samples (but don’t recurse)
- If can’t reach any more points, pick another core
sample, start new cluster.
- If newly picked point has not enough neighbors, label outlier
- Outliers can later be relabeled to belong to a cluster.
]
---
![:scale 100%](images/dbscan_animation.gif)

by David Sheehan <a href="https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/">dashee87.github.io</a>
???
This is from a blog post that shows you how DBSCAN works
iteratively with two different settings. So basically, here
in the left, epsilon is 1 and min points is 8 while on the
right, epsilon is 0.6, and min points is 6.

From the chart in the left, first we pick some in the blue
thing and we cover all of the blue cluster, and then
something in the green cluster and then the red and yellow.
And the rest all are just labeled noise because they don't
have eight neighbors.

In the case on the right, some of them have six neighbors
and so they’re labeled as their own clusters. You can see
here, in between they're randomly picked. So once we picked
these random points, they're labeled noise.

So there's no easy way to pick a direct upper bound on a
number of clusters, but you can run it for different volumes
of epsilon, and pick the one that has the right number of
clusters for you. So you can define that at priori.

---

# Illustration of Parameters

.center[
![:scale 70%](images/dbscan_illustration.png)
]

???
In this illustration, all the big points are core points,
all the small ones are not core points, and all the white
ones are noise. And so basically, if you decrease min
samples, the thing you do is all points that are smaller
than this becomes noise. So here, in the first row, one
cluster has four points, and the other cluster had three
points. So if I do min samples equal to three, they're both
clusters. So I do min samples to five, none of them are
clusters, they're labeled noise.

If you increase epsilon, you make more points neighbors. And
so basically, you're merging more and more clusters
together. Increasing epsilon creates fewer clusters, but
they're bigger. So you can see from in the second row, with
epsilon 1.5 you get three clusters, with epsilon equal two
you get two clusters, with epsilon three, you get just one
cluster.


FIXME add epsilon circles to points?

---
# Parameters

- The parameter eps determines what it means for points to be “close”. 

- Setting eps to be very small will mean that no points are core samples, and may lead to all points being labeled as noise. 

- Setting eps to be very large will result in all points forming a single cluster.

- The min_samples setting determines whether points in less dense regions will be labeled as outliers or as their own clusters. 

- min_samples determines the minimum cluster size.

---
# Pros and Cons

- Pro: Can learn arbitrary cluster shapes
- Pro: Can detect outliers
- Con: Needs two (non-obvious?) parameters to adjust

.center[
![:scale 50%](images/dbscan_pro_con.png)
]

???
What's nice about this is that it can learn arbitrary
cluster shapes. Even with agglomerative clustering, even if
you don't add, like some neighborhood graph topology on it,
agglomerative clustering would not be able to learn this.
DBSCAN has no problem with learning this since it only works
very locally.

Also, DBSCAN can detect outliers, which is kind of nice.
What's slightly less nice, but you need to adjust two
parameters and parameters are not directly related to the
number of clusters.

So it's not entirely clear to me, whether it is harder to
pick epsilon, or is it harder to pick number of clusters but
sometimes if an application asks for 10 clusters, that's
little bit harder so you need to rerun the algorithm with
multiple values of epsilon to figure out what epsilon should
it be to get you to 10 clusters.

This algorithm is pretty fast, because it only uses local
search. And it can find arbitrary cluster sizes. So that's
pretty nice.

Sometimes it can also give you very differently sized
clusters, similar to the agglomerative metrics. Often, I
find that I get like one big cluster and a couple of small
clusters, which is not usually what you want. If you want
various similar sized clusters, KMeans or ward is better.

The last algorithm I want to talk about is actually not a
clustering algorithm. But it's a probabilistic generative
model.


eps is related to number of clusters, so not much worse than kmeans or agglomerative
---
class: some-space
# Summary

- KMeans<br/>Classic, simple. Only convex cluster shapes, determined by cluster centers.
--

- Agglomerative<br/>Can take input topology into account, can produce hierarchy.
--

- DBSCAN<br/>Arbitrary cluster shapes, can detect outliers, often very different sized clusters.

???
The most classical clustering algorithm is KMeans. It's
quite simple. It's nice, because they have cluster centers,
and the cluster centers represent your clusters. But it's
very restricted in terms of the shape of each cluster
center. And it's sort of an approximate algorithm that
depends on initialization. So if you initialize differently,
you might get different results.

Agglomerative is kind of nice, because it can take the input
topology into account and it can produce the hierarchy. If
you have not that many data points looking at the dendogram
might be very helpful.

DBSCAN is great because it can find arbitrarily shaped
clusters and it can also detect outliers. The downside is
that you need to specify two parameters that are sort of not
so obvious to pick.

And finally, there's Gaussian mixture models, which are sort
of an upgrade of KMeans. They can model covariance. You can
do soft clustering, in that you can say how likely does it
comes from a particular component, so you don't hard assign
each point to a particular component. It gives you a
parametric density model, which is nice. But it can be hard
to fit if you want to fit the full covariance matrix. If you
only find the diagonal covariance matrix, it's not so bad.

Gaussian mixture models require multiple initializations
possibly, and they're like a local search procedure.



KMeans requires initialization, as do GMMs.
---

.center[
![:scale 95%](images/bim_2.png)
.smaller[
http://scikit-learn.org/dev/auto_examples/cluster/plot_cluster_comparison.html]
]

???
There's this example on the scikit-learn website which shows
a range of different clustering algorithms. So there’s
KMeans, affinity propagation, mean shift, spectral
clustering, ward, agglomerative clustering, DBSCAN, Birach
and Gaussian mixtures.

So you can sort of see what kind of problems they work with
or not work with. Though this is 2D and things behave very,
very differently in 2D than they behave in higher
dimensions.

All of these algorithms have parameters and adjusting these
parameters is really tricky. Another question that you
should ask yourself is, while some of these can get like
complex cluster shapes, like, DBSCAN and agglomerative were
able to get these two circles and these two bananas, KMeans
and the Gaussian mixtures can do that. But the question is,
is your data set actually going to look like this or is it
going to look like this?

I think in in the real world data sets are much more likely
to look something like this.


---
class: middle
# Questions ?


    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'magula',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
